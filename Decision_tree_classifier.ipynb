{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6RUVfiR3EvhOXmbDyWuAb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyen-nhat-mai/ensemble_project/blob/main/Decision_tree_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DECISION TREE CLASSIFIER**\n",
        "By Haiwei FU, Mengyu LIANG, Nhat Mai NGUYEN, Jinji SHEN and Vanshika SHARMA"
      ],
      "metadata": {
        "id": "h1Ky08SBpnOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "Rjjs4_qJqDxG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pdTEiEdhpfKC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Self-build DT classifier**"
      ],
      "metadata": {
        "id": "DDSOllQN1ueA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, data_left=None, data_right=None, gain=None, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.data_left = data_left\n",
        "        self.data_right = data_right\n",
        "        self.gain = gain\n",
        "        self.value = value"
      ],
      "metadata": {
        "id": "flFrGXyktkMd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " First, test with 2 parmeters: min_samples_split, max_depth"
      ],
      "metadata": {
        "id": "G5C0p_WCv-fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BuiltDecisionTree:\n",
        "\n",
        "    def __init__(self, min_samples_split=2, max_depth=5):\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.root = None\n",
        "    \n",
        "    @staticmethod\n",
        "    def _entropy(s):\n",
        "        '''\n",
        "        Calculates entropy from an array of integer values.\n",
        "        Input: s (list)\n",
        "        Output: entropy value (float)\n",
        "        '''\n",
        "        # Convert to integers to avoid runtime errors\n",
        "        counts = np.bincount(np.array(s, dtype=np.int64))\n",
        "        # Probabilities of each class label\n",
        "        percentages = counts / len(s)\n",
        "        # Caclulate entropy\n",
        "        entropy = 0\n",
        "        for pct in percentages:\n",
        "            if pct > 0:\n",
        "                entropy += pct * np.log2(pct)\n",
        "        return -entropy\n",
        "\n",
        "    def _information_gain(self, parent, left_child, right_child):\n",
        "        '''\n",
        "        Calculates information gain from a parent and two child nodes.\n",
        "        Input: \n",
        "        - parent: the parent node (list)\n",
        "        - left_child: left child of a parent (list)\n",
        "        - right_child: right child of a parent (list)\n",
        "        Output: information gain (float)\n",
        "        '''\n",
        "        num_left = len(left_child) / len(parent)\n",
        "        num_right = len(right_child) / len(parent)\n",
        "        return self._entropy(parent) - (num_left * self._entropy(left_child) + num_right * self._entropy(right_child))\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        '''\n",
        "        Calculates the best split for given features and target\n",
        "        Input:\n",
        "        -X: features (np.array)\n",
        "        -y: target (np.array or list)\n",
        "        Output: best split contain info such as feature_index, threshold, df_left, df_right, gain (dict)\n",
        "        '''\n",
        "        best_split = {}\n",
        "        best_info_gain = -1\n",
        "        n_rows, n_cols = X.shape\n",
        "        \n",
        "        # For every dataset feature\n",
        "        for f_idx in range(n_cols):\n",
        "            X_curr = X[:, f_idx]\n",
        "            # For every unique value of that feature\n",
        "            for threshold in np.unique(X_curr):\n",
        "                # Construct a dataset and split it to the left and right parts\n",
        "                # Left part includes records lower or equal to the threshold\n",
        "                # Right part includes records higher than the threshold\n",
        "                df = np.concatenate((X, y.reshape(1, -1).T), axis=1)\n",
        "                df_left = np.array([row for row in df if row[f_idx] <= threshold])\n",
        "                df_right = np.array([row for row in df if row[f_idx] > threshold])\n",
        "\n",
        "                # Do the calculation only if there's data in both subsets\n",
        "                if len(df_left) > 0 and len(df_right) > 0:\n",
        "                    # Obtain the value of the target variable for subsets\n",
        "                    y = df[:, -1]\n",
        "                    y_left = df_left[:, -1]\n",
        "                    y_right = df_right[:, -1]\n",
        "\n",
        "                    # Caclulate the information gain and save the split parameters if the current split if better then the previous best\n",
        "                    gain = self._information_gain(y, y_left, y_right)\n",
        "                    if gain > best_info_gain:\n",
        "                        best_split = {\n",
        "                            'feature_index': f_idx,\n",
        "                            'threshold': threshold,\n",
        "                            'df_left': df_left,\n",
        "                            'df_right': df_right,\n",
        "                            'gain': gain\n",
        "                        }\n",
        "                        best_info_gain = gain\n",
        "        return best_split\n",
        "    \n",
        "    def _build(self, X, y, depth=0):\n",
        "        '''\n",
        "        Recursive function, used to build a decision tree from the input data.\n",
        "        Input:\n",
        "        - X: features (np.array)\n",
        "        - y: target (np.array or list)\n",
        "        - depth: current depth of a tree, used as a stopping criteria\n",
        "        Output: Node\n",
        "        '''\n",
        "        n_rows, n_cols = X.shape\n",
        "        \n",
        "        # Check to see if a node should be leaf node\n",
        "        if n_rows >= self.min_samples_split and depth <= self.max_depth:\n",
        "            # Get the best split\n",
        "            best = self._best_split(X, y)\n",
        "            # If the split isn't pure\n",
        "            if best['gain'] > 0:\n",
        "                # Build a tree on the left (recursive)\n",
        "                left = self._build(\n",
        "                    X=best['df_left'][:, :-1], \n",
        "                    y=best['df_left'][:, -1], \n",
        "                    depth=depth + 1\n",
        "                )\n",
        "                # Build a tree on the right (recursive)\n",
        "                right = self._build(\n",
        "                    X=best['df_right'][:, :-1], \n",
        "                    y=best['df_right'][:, -1], \n",
        "                    depth=depth + 1\n",
        "                )\n",
        "                return Node(\n",
        "                    feature=best['feature_index'], \n",
        "                    threshold=best['threshold'], \n",
        "                    data_left=left, \n",
        "                    data_right=right, \n",
        "                    gain=best['gain']\n",
        "                )\n",
        "        # Leaf node - value is the most common target value \n",
        "        return Node(\n",
        "            value=Counter(y).most_common(1)[0][0])\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Train a decision tree classifier model.\n",
        "        Input:\n",
        "        -X: features (np.array)\n",
        "        -y: target (np.array or list)\n",
        "        Output: None\n",
        "        '''\n",
        "        # Call a recursive function to build the tree\n",
        "        self.root = self._build(X, y)\n",
        "        \n",
        "    def _predict(self, x, tree):\n",
        "        '''\n",
        "        Predict a single instance (tree traversal).\n",
        "        Input:\n",
        "        -x: single observation\n",
        "        -tree: built tree\n",
        "        Output: predicted class (float)\n",
        "        '''\n",
        "        # Leaf node\n",
        "        if tree.value != None:\n",
        "            return tree.value\n",
        "        feature_value = x[tree.feature]\n",
        "        \n",
        "        # Go to the left\n",
        "        if feature_value <= tree.threshold:\n",
        "            return self._predict(x=x, tree=tree.data_left)\n",
        "        \n",
        "        # Go to the right\n",
        "        if feature_value > tree.threshold:\n",
        "            return self._predict(x=x, tree=tree.data_right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Classify new instances.\n",
        "        -X: features (np.array)\n",
        "        Output: predicted classes (np.array)\n",
        "        '''\n",
        "        # Call the _predict() function for every observation\n",
        "        return [self._predict(x, self.root) for x in X]"
      ],
      "metadata": {
        "id": "pW4M2HnTvb9W"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test and compare**\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html"
      ],
      "metadata": {
        "id": "kblGx7zY1kbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import wine data for testing\n",
        "wine = load_wine()\n",
        "data=pd.DataFrame(data=np.c_[wine['data'],wine['target']],columns=wine['feature_names']+['target'])\n",
        "print(\"Target distinct classes:\", data['target'].unique())\n",
        "print(\"\")\n",
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awpk-ILU1Thr",
        "outputId": "9031699d-ade5-4528-e353-921a1d3fda56"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target distinct classes: [0. 1. 2.]\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 178 entries, 0 to 177\n",
            "Data columns (total 14 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   alcohol                       178 non-null    float64\n",
            " 1   malic_acid                    178 non-null    float64\n",
            " 2   ash                           178 non-null    float64\n",
            " 3   alcalinity_of_ash             178 non-null    float64\n",
            " 4   magnesium                     178 non-null    float64\n",
            " 5   total_phenols                 178 non-null    float64\n",
            " 6   flavanoids                    178 non-null    float64\n",
            " 7   nonflavanoid_phenols          178 non-null    float64\n",
            " 8   proanthocyanins               178 non-null    float64\n",
            " 9   color_intensity               178 non-null    float64\n",
            " 10  hue                           178 non-null    float64\n",
            " 11  od280/od315_of_diluted_wines  178 non-null    float64\n",
            " 12  proline                       178 non-null    float64\n",
            " 13  target                        178 non-null    float64\n",
            "dtypes: float64(14)\n",
            "memory usage: 19.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train & test sets\n",
        "X = wine['data']\n",
        "y = wine['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "74KlT4gD22-5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with built-in DT\n",
        "model = BuiltDecisionTree()\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "print(\"Predicted classes: \", preds)\n",
        "print(\"True classes\",y_test)\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8l5mDs-312J",
        "outputId": "8beb9174-8819-4ac2-f70c-b85bc833b9af"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes:  [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0]\n",
            "True classes [0 0 2 0 1 0 1 2 1 2 0 2 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0]\n",
            "Test accuracy:  0.9166666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with sklearn DT\n",
        "model = DecisionTreeClassifier(min_samples_split=2, max_depth=5) # apply the default parameters of self-built DT\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "print(\"Predicted classes: \", preds)\n",
        "print(\"True classes\",y_test)\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEYrGAFW6lhE",
        "outputId": "daf19435-fdb8-4050-8c83-0011dd1e4f27"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes:  [0 0 2 0 1 0 1 2 1 2 1 1 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0]\n",
            "True classes [0 0 2 0 1 0 1 2 1 2 0 2 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0]\n",
            "Test accuracy:  0.9444444444444444\n"
          ]
        }
      ]
    }
  ]
}
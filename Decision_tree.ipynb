{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyen-nhat-mai/ensemble_project/blob/main/Decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DECISION TREE CLASSIFIER and REGRESSOR**\n",
        "By Haiwei FU, Mengyu LIANG, Nhat Mai NGUYEN, Jinji SHEN and Vanshika SHARMA"
      ],
      "metadata": {
        "id": "h1Ky08SBpnOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "Rjjs4_qJqDxG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdTEiEdhpfKC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Self-build DT**"
      ],
      "metadata": {
        "id": "DDSOllQN1ueA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 1 (only classification)"
      ],
      "metadata": {
        "id": "1eamzRNnCXHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, data_left=None, data_right=None, gain=None, value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.data_left = data_left\n",
        "        self.data_right = data_right\n",
        "        self.gain = gain\n",
        "        self.value = value"
      ],
      "metadata": {
        "id": "flFrGXyktkMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " First, test with 2 parmeters: min_samples_split, max_depth"
      ],
      "metadata": {
        "id": "G5C0p_WCv-fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BuiltDecisionTree:\n",
        "\n",
        "    def __init__(self, min_samples_split=2, max_depth=5):\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.root = None\n",
        "    \n",
        "    @staticmethod\n",
        "    def _entropy(s):\n",
        "        '''\n",
        "        Calculates entropy from an array of integer values.\n",
        "        Input: s (list)\n",
        "        Output: entropy value (float)\n",
        "        '''\n",
        "        # Convert to integers to avoid runtime errors\n",
        "        counts = np.bincount(np.array(s, dtype=np.int64))\n",
        "        # Probabilities of each class label\n",
        "        percentages = counts / len(s)\n",
        "        # Caclulate entropy\n",
        "        entropy = 0\n",
        "        for pct in percentages:\n",
        "            if pct > 0:\n",
        "                entropy += pct * np.log2(pct)\n",
        "        return -entropy\n",
        "\n",
        "    def _information_gain(self, parent, left_child, right_child):\n",
        "        '''\n",
        "        Calculates information gain from a parent and two child nodes.\n",
        "        Input: \n",
        "        - parent: the parent node (list)\n",
        "        - left_child: left child of a parent (list)\n",
        "        - right_child: right child of a parent (list)\n",
        "        Output: information gain (float)\n",
        "        '''\n",
        "        num_left = len(left_child) / len(parent)\n",
        "        num_right = len(right_child) / len(parent)\n",
        "        return self._entropy(parent) - (num_left * self._entropy(left_child) + num_right * self._entropy(right_child))\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        '''\n",
        "        Calculates the best split for given features and target\n",
        "        Input:\n",
        "        -X: features (np.array)\n",
        "        -y: target (np.array or list)\n",
        "        Output: best split contain info such as feature_index, threshold, df_left, df_right, gain (dict)\n",
        "        '''\n",
        "        best_split = {}\n",
        "        best_info_gain = -1\n",
        "        n_rows, n_cols = X.shape\n",
        "        \n",
        "        # For every dataset feature\n",
        "        for f_idx in range(n_cols):\n",
        "            X_curr = X[:, f_idx]\n",
        "            # For every unique value of that feature\n",
        "            for threshold in np.unique(X_curr):\n",
        "                # Construct a dataset and split it to the left and right parts\n",
        "                # Left part includes records lower or equal to the threshold\n",
        "                # Right part includes records higher than the threshold\n",
        "                df = np.concatenate((X, y.reshape(1, -1).T), axis=1)\n",
        "                df_left = np.array([row for row in df if row[f_idx] <= threshold])\n",
        "                df_right = np.array([row for row in df if row[f_idx] > threshold])\n",
        "\n",
        "                # Do the calculation only if there's data in both subsets\n",
        "                if len(df_left) > 0 and len(df_right) > 0:\n",
        "                    # Obtain the value of the target variable for subsets\n",
        "                    y = df[:, -1]\n",
        "                    y_left = df_left[:, -1]\n",
        "                    y_right = df_right[:, -1]\n",
        "\n",
        "                    # Caclulate the information gain and save the split parameters if the current split if better then the previous best\n",
        "                    gain = self._information_gain(y, y_left, y_right)\n",
        "                    if gain > best_info_gain:\n",
        "                        best_split = {\n",
        "                            'feature_index': f_idx,\n",
        "                            'threshold': threshold,\n",
        "                            'df_left': df_left,\n",
        "                            'df_right': df_right,\n",
        "                            'gain': gain\n",
        "                        }\n",
        "                        best_info_gain = gain\n",
        "        return best_split\n",
        "    \n",
        "    def _build(self, X, y, depth=0):\n",
        "        '''\n",
        "        Recursive function, used to build a decision tree from the input data.\n",
        "        Input:\n",
        "        - X: features (np.array)\n",
        "        - y: target (np.array or list)\n",
        "        - depth: current depth of a tree, used as a stopping criteria\n",
        "        Output: Node\n",
        "        '''\n",
        "        n_rows, n_cols = X.shape\n",
        "        \n",
        "        # Check to see if a node should be leaf node\n",
        "        if n_rows >= self.min_samples_split and depth <= self.max_depth:\n",
        "            # Get the best split\n",
        "            best = self._best_split(X, y)\n",
        "            # If the split isn't pure\n",
        "            if best['gain'] > 0:\n",
        "                # Build a tree on the left (recursive)\n",
        "                left = self._build(\n",
        "                    X=best['df_left'][:, :-1], \n",
        "                    y=best['df_left'][:, -1], \n",
        "                    depth=depth + 1\n",
        "                )\n",
        "                # Build a tree on the right (recursive)\n",
        "                right = self._build(\n",
        "                    X=best['df_right'][:, :-1], \n",
        "                    y=best['df_right'][:, -1], \n",
        "                    depth=depth + 1\n",
        "                )\n",
        "                return Node(\n",
        "                    feature=best['feature_index'], \n",
        "                    threshold=best['threshold'], \n",
        "                    data_left=left, \n",
        "                    data_right=right, \n",
        "                    gain=best['gain']\n",
        "                )\n",
        "        # Leaf node - value is the most common target value \n",
        "        return Node(\n",
        "            value=Counter(y).most_common(1)[0][0])\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Train a decision tree classifier model.\n",
        "        Input:\n",
        "        -X: features (np.array)\n",
        "        -y: target (np.array or list)\n",
        "        Output: None\n",
        "        '''\n",
        "        # Call a recursive function to build the tree\n",
        "        self.root = self._build(X, y)\n",
        "        \n",
        "    def _predict(self, x, tree):\n",
        "        '''\n",
        "        Predict a single instance (tree traversal).\n",
        "        Input:\n",
        "        -x: single observation\n",
        "        -tree: built tree\n",
        "        Output: predicted class (float)\n",
        "        '''\n",
        "        # Leaf node\n",
        "        if tree.value != None:\n",
        "            return tree.value\n",
        "        feature_value = x[tree.feature]\n",
        "        \n",
        "        # Go to the left\n",
        "        if feature_value <= tree.threshold:\n",
        "            return self._predict(x=x, tree=tree.data_left)\n",
        "        \n",
        "        # Go to the right\n",
        "        if feature_value > tree.threshold:\n",
        "            return self._predict(x=x, tree=tree.data_right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Classify new instances.\n",
        "        -X: features (np.array)\n",
        "        Output: predicted classes (np.array)\n",
        "        '''\n",
        "        # Call the _predict() function for every observation\n",
        "        return [self._predict(x, self.root) for x in X]\n",
        "    \n"
      ],
      "metadata": {
        "id": "pW4M2HnTvb9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 2 - for both classification and regression tasks"
      ],
      "metadata": {
        "id": "jvLcgLCqCbVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#class to control tree node\n",
        "class Node:\n",
        "    #initializer\n",
        "    def __init__(self):\n",
        "        self.__Bs    = None\n",
        "        self.__Bf    = None\n",
        "        self.__left  = None\n",
        "        self.__right = None\n",
        "        self.leafv   = None\n",
        "    #set the split,feature parameters for this node\n",
        "    def set_params(self,Bs,Bf):\n",
        "        self.__Bs = Bs\n",
        "        self.__Bf = Bf\n",
        "        \n",
        "    #get the split,feature parameters for this node\n",
        "    def get_params(self):\n",
        "        return(self.__Bs,self.__Bf)    \n",
        "        \n",
        "    #set the left/right children nodes for this current node\n",
        "    def set_children(self,left,right):\n",
        "        self.__left  = left\n",
        "        self.__right = right\n",
        "        \n",
        "    #get the left child node\n",
        "    def get_left_node(self):\n",
        "        return(self.__left)\n",
        "    \n",
        "    #get the right child node\n",
        "    def get_right_node(self):\n",
        "        return(self.__right)\n",
        "\n",
        "#base class to encompass the decision tree algorithm\n",
        "class DecisionTree:\n",
        "    #initializer\n",
        "    def __init__(self,max_depth=None,min_samples_split=2):\n",
        "        self.tree              = None\n",
        "        self.max_depth         = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "    \n",
        "    #protected function to define the impurity\n",
        "    def _impurity(self,D):\n",
        "         pass\n",
        "        \n",
        "    #protected function to compute the value at a leaf node\n",
        "    def _leaf_value(self,D):\n",
        "         pass\n",
        "    \n",
        "    #private recursive function to grow the tree during training\n",
        "    def __grow(self,node,D,level):       \n",
        "        #are we in a leaf node? let's do some check...\n",
        "        depth = (self.max_depth is None) or (self.max_depth >= (level+1))\n",
        "        msamp = (self.min_samples_split <= D.shape[0])\n",
        "        n_cls = np.unique(D[:,-1]).shape[0] != 1\n",
        "        \n",
        "        #not a leaf node\n",
        "        if depth and msamp and n_cls:\n",
        "        \n",
        "            #initialize the function parameters\n",
        "            ip_node = None\n",
        "            feature = None\n",
        "            split   = None\n",
        "            left_D  = None\n",
        "            right_D = None\n",
        "            #iterrate through the possible feature/split combinations\n",
        "            for f in range(D.shape[1]-1):\n",
        "                for s in np.unique(D[:,f]):\n",
        "                    #for the current (f,s) combination, split the dataset\n",
        "                    D_l = D[D[:,f]<=s]\n",
        "                    D_r = D[D[:,f]>s]\n",
        "                    #ensure we have non-empty arrays\n",
        "                    if D_l.size and D_r.size:\n",
        "                        #calculate the impurity\n",
        "                        ip  = (D_l.shape[0]/D.shape[0])*self._impurity(D_l) + (D_r.shape[0]/D.shape[0])*self._impurity(D_r)\n",
        "                        #now update the impurity and choice of (f,s)\n",
        "                        if (ip_node is None) or (ip < ip_node):\n",
        "                            ip_node = ip\n",
        "                            feature = f\n",
        "                            split   = s\n",
        "                            left_D  = D_l\n",
        "                            right_D = D_r\n",
        "            #set the current node's parameters\n",
        "            node.set_params(split,feature)\n",
        "            #declare child nodes\n",
        "            left_node  = Node()\n",
        "            right_node = Node()\n",
        "            node.set_children(left_node,right_node)\n",
        "            #investigate child nodes\n",
        "            self.__grow(node.get_left_node(),left_D,level+1)\n",
        "            self.__grow(node.get_right_node(),right_D,level+1)\n",
        "                        \n",
        "        #is a leaf node\n",
        "        else:\n",
        "            \n",
        "            #set the node value & return\n",
        "            node.leafv = self._leaf_value(D)\n",
        "            return\n",
        "        \n",
        "    #private recursive function to traverse the (trained) tree\n",
        "    def __traverse(self,node,Xrow):\n",
        "        #check if we're in a leaf node?\n",
        "        if node.leafv is None:\n",
        "            #get parameters at the node\n",
        "            (s,f) = node.get_params()\n",
        "            #decide to go left or right?\n",
        "            if (Xrow[f] <= s):\n",
        "                return(self.__traverse(node.get_left_node(),Xrow))\n",
        "            else:\n",
        "                return(self.__traverse(node.get_right_node(),Xrow))\n",
        "        else:\n",
        "            #return the leaf value\n",
        "            return(node.leafv)\n",
        "      \n",
        "    #train the tree model\n",
        "    def train(self,Xin,Yin):\n",
        "        #prepare the input data\n",
        "        D = np.concatenate((Xin,Yin.reshape(-1,1)),axis=1)\n",
        "        #set the root node of the tree\n",
        "        self.tree = Node()\n",
        "        #build the tree\n",
        "        self.__grow(self.tree,D,1)\n",
        "        \n",
        "    #make predictions from the trained tree\n",
        "    def predict(self,Xin):\n",
        "        #iterrate through the rows of Xin\n",
        "        p = []\n",
        "        for r in range(Xin.shape[0]):\n",
        "            p.append(self.__traverse(self.tree,Xin[r,:]))\n",
        "        #return predictions\n",
        "        return(np.array(p).flatten())"
      ],
      "metadata": {
        "id": "tel1ti9AYIGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree Classifier\n",
        "class DecisionTreeClassifier(DecisionTree):\n",
        "    #initializer\n",
        "    def __init__(self,max_depth=2,min_samples_split=5,loss='gini'):\n",
        "        DecisionTree.__init__(self,max_depth,min_samples_split)\n",
        "        self.loss = loss   \n",
        "    \n",
        "    #private function to define the gini impurity\n",
        "    def __gini(self,D):\n",
        "        #initialize the output\n",
        "        G = 0\n",
        "        #iterrate through the unique classes\n",
        "        for c in np.unique(D[:,-1]):\n",
        "            #compute p for the current c\n",
        "            p = D[D[:,-1]==c].shape[0]/D.shape[0]\n",
        "            #compute term for the current c\n",
        "            G += p*(1-p)\n",
        "        #return gini impurity\n",
        "        return(G)\n",
        "    \n",
        "    #private function to define the shannon entropy\n",
        "    def __entropy(self,D):\n",
        "        #initialize the output\n",
        "        H = 0\n",
        "        #iterrate through the unique classes\n",
        "        for c in np.unique(D[:,-1]):\n",
        "            #compute p for the current c\n",
        "            p = D[D[:,-1]==c].shape[0]/D.shape[0]\n",
        "            #compute term for the current c\n",
        "            H -= p*np.log2(p)\n",
        "        #return entropy\n",
        "        return(H)\n",
        "    \n",
        "    #protected function to define the impurity\n",
        "    def _impurity(self,D):\n",
        "        #use the selected loss function to calculate the node impurity\n",
        "        ip = None\n",
        "        if self.loss == 'gini':\n",
        "            ip = self.__gini(D)\n",
        "        elif self.loss == 'entropy':\n",
        "            ip = self.__entropy(D)\n",
        "        #return results\n",
        "        return(ip)\n",
        "    \n",
        "    #protected function to compute the value at a leaf node\n",
        "    def _leaf_value(self,D):\n",
        "         return(stats.mode(D[:,-1])[0])"
      ],
      "metadata": {
        "id": "lkFQzdEke1y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree Regressor\n",
        "class DecisionTreeRegressor(DecisionTree):\n",
        "    #initializer\n",
        "    def __init__(self,max_depth=None,min_samples_split=2,loss='mse'):\n",
        "        DecisionTree.__init__(self,max_depth,min_samples_split)\n",
        "        self.loss = loss   \n",
        "    \n",
        "    #private function to define the mean squared error\n",
        "    def __mse(self,D):\n",
        "        #compute the mean target for the node\n",
        "        y_m = np.mean(D[:,-1])\n",
        "        #compute the mean squared error wrt the mean\n",
        "        E = np.sum((D[:,-1] - y_m)**2)/D.shape[0]\n",
        "        #return mse\n",
        "        return(E)\n",
        "    \n",
        "    #private function to define the mean absolute error\n",
        "    def __mae(self,D):\n",
        "        #compute the mean target for the node\n",
        "        y_m = np.mean(D[:,-1])\n",
        "        #compute the mean absolute error wrt the mean\n",
        "        E = np.sum(np.abs(D[:,-1] - y_m))/D.shape[0]\n",
        "        #return mae\n",
        "        return(E)\n",
        "    \n",
        "    #protected function to define the impurity\n",
        "    def _impurity(self,D):\n",
        "        #use the selected loss function to calculate the node impurity\n",
        "        ip = None\n",
        "        if self.loss == 'mse':\n",
        "            ip = self.__mse(D)\n",
        "        elif self.loss == 'mae':\n",
        "            ip = self.__mae(D)\n",
        "        #return results\n",
        "        return(ip)\n",
        "    \n",
        "    #protected function to compute the value at a leaf node\n",
        "    def _leaf_value(self,D):\n",
        "         return(np.mean(D[:,-1]))"
      ],
      "metadata": {
        "id": "4uSe6LTGgUWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test and compare for classification task**\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html"
      ],
      "metadata": {
        "id": "kblGx7zY1kbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Version 1**"
      ],
      "metadata": {
        "id": "KWRpMIwME5oL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import wine data for testing\n",
        "wine = load_wine()\n",
        "data=pd.DataFrame(data=np.c_[wine['data'],wine['target']],columns=wine['feature_names']+['target'])\n",
        "print(\"Target distinct classes:\", data['target'].unique())\n",
        "print(\"\")\n",
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awpk-ILU1Thr",
        "outputId": "42b95572-7b98-4ffb-a331-6619ec7f4886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target distinct classes: [0. 1. 2.]\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 178 entries, 0 to 177\n",
            "Data columns (total 14 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   alcohol                       178 non-null    float64\n",
            " 1   malic_acid                    178 non-null    float64\n",
            " 2   ash                           178 non-null    float64\n",
            " 3   alcalinity_of_ash             178 non-null    float64\n",
            " 4   magnesium                     178 non-null    float64\n",
            " 5   total_phenols                 178 non-null    float64\n",
            " 6   flavanoids                    178 non-null    float64\n",
            " 7   nonflavanoid_phenols          178 non-null    float64\n",
            " 8   proanthocyanins               178 non-null    float64\n",
            " 9   color_intensity               178 non-null    float64\n",
            " 10  hue                           178 non-null    float64\n",
            " 11  od280/od315_of_diluted_wines  178 non-null    float64\n",
            " 12  proline                       178 non-null    float64\n",
            " 13  target                        178 non-null    float64\n",
            "dtypes: float64(14)\n",
            "memory usage: 19.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train & test sets\n",
        "X = wine['data']\n",
        "y = wine['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "74KlT4gD22-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with built-in DT\n",
        "model = BuiltDecisionTree()\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "print(\"Predicted classes: \", preds)\n",
        "print(\"True classes\",y_test)\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8l5mDs-312J",
        "outputId": "dc0ef11f-339a-4289-ee14-554bd10e9391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes:  [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0]\n",
            "True classes [0 0 2 0 1 0 1 2 1 2 0 2 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0]\n",
            "Test accuracy:  0.9166666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Version 2**"
      ],
      "metadata": {
        "id": "t4_-bkCTE850"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_wine()\n",
        "X    = data.data\n",
        "y    = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "OPCIe18LftTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = DecisionTreeClassifier(min_samples_split=2, max_depth=5)\n",
        "clf.train(X_train,y_train)\n",
        "preds = clf.predict(X_test)\n",
        "print(\"Predicted classes: \", preds)\n",
        "print(\"True classes\",y_test)\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdC--1wKEz5M",
        "outputId": "bee7cf61-d866-4e65-ddc5-a8350b2cae13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes:  [0. 0. 2. 0. 1. 0. 1. 2. 1. 2. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 2.\n",
            " 2. 2. 1. 1. 1. 0. 0. 1. 2. 0. 0. 0.]\n",
            "True classes [0 0 2 0 1 0 1 2 1 2 0 2 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0]\n",
            "Test accuracy:  0.9166666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r6irbJ46gD-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with sklearn DT\n",
        "model = DecisionTreeClassifier(min_samples_split=2, max_depth=5) # apply the default parameters of self-built DT\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_test)\n",
        "print(\"Predicted classes: \", preds)\n",
        "print(\"True classes\",y_test)\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEYrGAFW6lhE",
        "outputId": "daf19435-fdb8-4050-8c83-0011dd1e4f27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes:  [0 0 2 0 1 0 1 2 1 2 1 1 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0]\n",
            "True classes [0 0 2 0 1 0 1 2 1 2 0 2 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0]\n",
            "Test accuracy:  0.9444444444444444\n"
          ]
        }
      ]
    }
  ]
}